# ml-from-scratch-transformer
# ğŸ§  Transformer From Scratch

This project is a **from-scratch implementation** of the original [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) paper using **pure PyTorch**. It's part of my ongoing GitHub series: [`ml-from-scratch-xyz`](https://github.com/amansahu278?tab=repositories&q=ml-from-scratch).

> ğŸ”¬ Goal: To understand and re-implement the Transformer architecture without relying on high-level libraries like HuggingFace or PyTorch Lightning.

---

## ğŸ“š Features

- Encoder & Decoder architecture
- Multi-Head Attention with masking
- Scaled Dot-Product Attention
- Sinusoidal positional encodings
- Position-wise Feedforward Networks
- Layer Normalization
- Autoregressive `generate()` method
- Optional support for returning:
  - `attention_probs`
  - `past_key_values`

---

## ğŸ—ï¸ Project Structure

```bash
.
â”œâ”€â”€ transformer.py         # Main model class
â”œâ”€â”€ encoder.py             # Encoder & EncoderLayer
â”œâ”€â”€ decoder.py             # Decoder &DecoderLayer
â”œâ”€â”€ attention.py           # MHA & Scaled Dot Product
â”œâ”€â”€ positional_encoding.py # Sinusoidal encoding
â”œâ”€â”€ utils.py               # Position-wise FFN, Linear, LayerNorm
â”œâ”€â”€ main.py               # Training loop
â””â”€â”€ README.md
